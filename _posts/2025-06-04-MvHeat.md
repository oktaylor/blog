---
layout: post
title: "EventHPE: Event-based 3D Human Pose and Shape Estimation"
date: 2024-03-18 20:00:00 +0900
categories: [Paper Review]
tags:
  [
    Paper review, Event, Datasets, Human pose estimation, Optical flow, SMPL, 3D
  ]
math: true
image:
  path: /assets/img/2024-03-18-EventHPE/Untitled 3.png
#   alt: 대표 이미지 캡션---
---
> **CVPR, 2025**<br/>
> [**[Paper](https://arxiv.org/abs/2412.06647)**]
> [**[Code](https://github.com/Event-AHU/OpenEvDET)**]

## Contributions
- **MoE (Misture of Experts) heat conduction** 기반의 object detection 알고리즘 제안 (MvHeat-DET)
- Event data embedding을 위한 stem network → MoE-HCO block 통과
- **EvDET200K** event object detection dataset
> Event representation : frame

## Introduction
- 기존 event 기반 object detector의 문제점
  1. CNN 혹은 Transformer 기반 backbone에 의존함. (CNN은 global 학습에 취약, ViT는 계산 복잡도 & interpretability)
  2. SNN 기반 event encoding : 성능이 ANN기반 대비 떨어짐
- Heat Conduction Operator (HCO)
  - [vHeat](https://arxiv.org/abs/2405.16555) 모델에서 제안
  - image patch를 heat source로 가정, thermal energy diffusion으로 vision 문제를 해결
  - 2D DCT, IDCT 활용 → 계산 복잡도 줄임, but 이는 visual data에 최적화 되어있지 않음
- **Contributions**

## MvHeat-DET
### Preliminaries : Physical Heat Conduction
- 논문의 모델은 2-dim heat conduction equation을 기반으로 설계됨

2-dim heat conduction equation (k는 thermal diffusivity) : 
$$\frac{\partial\left(x,y,t\right)}{\partial t}=k\left(\frac{\partial^2u\left(x,y,t\right)}{\partial x^2}+\frac{\partial^2u\left(x,y,t\right)}{\partial y^2}\right)=k(u_{xx}+u_{yy})$$
푸리에 변환을 취하면,
$$\mathcal{F}\frac{\partial\left(x,y,t\right)}{\partial t}=k\mathcal{F}\left(u_{xx}+u_{yy}\right)$$
$$\frac{\partial\hat{u}\left(v_x, v_y, v_t\right)}{\partial t}=-k\left(v_x^2+v_y^2\right)\hat{u}(v_x,v_y,t)$$
일반해는 다음과 같음
$$\hat{u}\left(v_x, v_y, v_t\right)=\hat{f}\left(v_x, v_y\right)e^{-k\left(v_x^2+v_y^2\right)t}$$
Inverse fourier 과정을 거치면,
$$u\left(x,y,t\right)=\mathcal{F}^{-1}\left(\hat{f}(v_x,v_y)e^{-k\left(v_x^2+v_y^2\right)t}\right)$$
모델은 위 식을 변환하는 과정을 거치면서 학습을 진행함

### Overview
![Figure 2.](/assets/img/2025-06-04-MvHeat/Untitled-1.png)
- Stem network : Event frame으로 event embedding을 제작
- MoE-HCO block : DFT-IDFT (Cosine), DCT-IDCT (Fourier), HT-IHT (Haar) 활용
- Policy network
- Gumbel softmax for expert selection
- Frequency embeddings (FEs) : k 예측
- IoU-based query selection : 최종 detection에서 key token 검색

### Mixture of Experts Heat Conduction Operation
- Backbone으로 활용
- MoE-HCO Layer > MoE-HCO Block > MHCO
- 레이어 자체는 ViT와 유사

**Process**
- Embedded input이 DWConv를 통과 → temperature distribution로 확장 ($U_0$)
- Frequency embeddings (FEs)로 k 예측 (FEs는 learnable parameter)
- Selection mechanism (Policy net + Gumbel softmax) → expert 선택 (DFT, DCT, HT 중 1개)
$$U_t =\mathcal{F}^{-1}\left(\mathcal{F}\left(U_0\right)e^{-k\left(v^2_x+v^2_y\right)t}\right), \quad U_t =\mathcal{C}^{-1}\left(\mathcal{C}\left(U_0\right)e^{-k\left(v^2_x+v^2_y\right)t}\right), \quad U_t =\mathcal{H}^{-1}\left(\mathcal{H}\left(U_0\right)e^{-k\left(v^2_x+v^2_y\right)t}\right)$$
- 이 때, 이미지는 유한한 크기이므로 Neumann 경계 조건 설정
$$\frac{\partial u\left(x,y,t\right)}{\partial \mathbf{n}}=0, \forall\left(x,y\right) \in D, t>0$$

**MoE for event domain**
- low-motion objects나 sparse한 경우
  - 빈 공간이 많이 생김 → 원본 이미지를 여러 패치로 나누면 roi가 단일 patch 내에 존재하는 경우가 많음
  - 따라서, patch 내부만 처리하고, patch 간 interaction은 무시 → **DCT와 HT**를 각 patch 내부에 적용
- high-motion object나 dense scene의 경우
  - patch 사이의 interaction 고려해야 함 → **DFT**를 활용해 patch 간 정보 교환
  - 
**Loss function with IoU-based Query Selection (IQS)**
- 기존 [DETR](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf)에서 top-k query 선택 시 classification과 IoU score 차이가 높은 문제점을 수정
- classification loss에 IoU를 포함시켜 classification과 IoU score가 동시에 높을 수 있도록 함
$$\mathcal{L}\left(y, \hat{y}\right)=\mathcal{L}_{bbox}\left(b,\hat{b}\right)+\mathcal{L}_{cls}\left(IoU,c,\hat{c}\right)$$

### Results
![Untitled](/assets/img/2025-06-04-MvHeat/Untitled-2.png)
FPS가 58이라 실시간은 무리인 듯 함

![Untitled](/assets/img/2025-06-04-MvHeat/Untitled-3.png)
vHeat의 feature map

![ablation](/assets/img/2025-06-04-MvHeat/Untitled-4.png)
k가 learnable parameter인 것과 FEs를 적용한 것의 차이는 코드를 봐야 할 것 같음
