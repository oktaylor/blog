---
layout: post
title: "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention"
date: 2025-06-05 23:00:00 +0900
categories: [Paper Review]
tags:
  [
    Paper review, Event camera, Object detection, SNN
  ]
math: true
image:
  path: /assets/img/2025-06-05-EvSnnAnn/Untitled-1.png
#   alt: 대표 이미지 캡션---
---
> **CVPR, 2025**<br/>
> [**[Paper](https://arxiv.org/abs/2403.10173)**]
> [**[Project](https://soikathasanahmed.github.io/hybrid/)**]

## Contributions
- **Hybrid 형태의 SNN-ANN** event object detection 모델 제안
- SNN과 ANN 사이를 연결하는 **bridge module $\beta_{asab}$** 제안
- ANN 내부에 RNN 구조(DWConvLSTM)를 결합하여, SNN+RNN 구조로 변형, 긴 시간 범위의 temporal 정보를 처리

## Method

### Process
![fig1](/assets/img/2025-06-05-EvSnnAnn/Untitled-1.png)
![fig1](/assets/img/2025-06-05-EvSnnAnn/Untitled-1-1.png){: width="70%" height="70%"}
1. Event processing
2. SNN block : high temporal resolution에서 low-level feature 추출
3. **Attention-based SNN-ANN Bridge Module → 핵심 모듈**
4. ANN block & DWConvLSTM : 넓은 시간 구간(slower timescale)에서 dense activation 사용
5. Head : 최종 detection

### Event processing
- 논문의 모델은 [RVT](https://openaccess.thecvf.com/content/CVPR2023/html/Gehrig_Recurrent_Vision_Transformers_for_Object_Detection_With_Event_Cameras_CVPR_2023_paper.html)에서 제안했던 event processing을 사용
  - Histogram 방식과 voxel grid 방식을 섞어 놓은 것으로 보임
  - Event stream 전체 구간에서 세분화된 시간 구간과 polarity를 쪼개서 4차원 tensor로 변환 → $\left[t_{k-1},t_k\right]\in\mathbb{R}^{T\times2\times H\times W}$

    $$Events\left[t_{k-1},t_k\right]\left(t,p,x,y\right)=\sum_{e_n\in\mathcal{E}}\delta\left(p-p_n\right)\delta\left(x-x_n\right)\delta\left(y-y_n\right)\delta\left(t-t^{'}_n\right),\quad t^{'}_n=\left\lfloor\frac{t_n-t_a}{t_b-t_a}\cdot T\right\rfloor$$

  - 논문에서는 k와 a,b를 혼용한 듯 함

### SNN block
![fig4-1](/assets/img/2025-06-05-EvSnnAnn/Untitled-2.png){: width="40%" height="40%"}
- convolution, batch norm, parametric leaky integration and fire(PLIF)로 구성
- $\tau=sigmoid(w)^{-1}$, input이 $X[t]$일 때 막전위:

  $$V[t]=V[t-1]+\frac{1}{\tau}\left(x[t]-(V[t-1]-V_{reset})\right)$$

- 출력 : $\textbf{E}_{spike}\in\mathbb{R}^{T\times C\times H'\times W'}$

### Attention-based SNN-ANN Bridge Module
![fig2](/assets/img/2025-06-05-EvSnnAnn/Untitled-3.png)
- $\textbf{E}_{spike}$ → $A_{in}\in\mathbb{R}^{C\times T\times H'\times W'}$

**Spatial-aware temporal (SAT) attention**
- Time-wise separable defromable convolutions ($\Phi_{tsdc}$)
  ![fig5](/assets/img/2025-06-05-EvSnnAnn/Untitled-4.png)
  - $A_c\in\mathbb{R}^{T\times H'\times W'}$: channel-wise하게 변경
    - Event camera는 object가 움직일 때 edge가 생김
    - 시간이 지남에 따라 object가 이동할 때 각 시간 bin마다 event의 위치가 달라짐
    - 따라서 feature에 대응하는 channel 단위로 grouping하는 방식을 사용
  - deformable convolution : event와 같은 irregular representation에 강함
  - 채널 별로 시간에 따라 변하는 공간 정보 추출
  - 출력 : $A_{sc}$
- Temporal attention ($\Phi_{ta}$)
  - $A_{sc}$를 $1\times 1$ convolution를 통과시켜 q, k, v로 나눈 후 attention 수행
  - $1\times 1$ convolution + ReLU
  - 어떤 시점이 중요한지를 판단
  - 츨력 : $A_{out}\in\mathbb{R}^{C\times H'\times W'}$

**Event-rate spatial (ERS) attention**
- T 차원으로 summation : 전체 시간 동안의 spatial spike 계산
- Sigmoid : 공간적으로 중요한 영역 강조
- SAT 출력 $A_{out}$과 hadamard product
- 출력 : $F_{out}\in\mathbb{R}^{C\times H'\times W'}$

### ANN block
![fig4-2](/assets/img/2025-06-05-EvSnnAnn/Untitled-5.png){: width="40%" height="40%"}
- convolution, normalization, ReLU로 구성
- detail한 high-level spatial feature 추출
- 
### Results
- Dataset : Gen1 Automotive Detection, Gen4 Automotive Detection 
![tab1](/assets/img/2025-06-05-EvSnnAnn/Untitled-6.png){: width="80%" height="80%"}
![tab3](/assets/img/2025-06-05-EvSnnAnn/Untitled-7.png){: width="70%" height="70%"}
WDConvLSTM은 자세히 나와있지 않음
